import os
import sys
from openai import OpenAI

# 1. é…ç½®è¿æ¥ï¼šæŒ‡å‘ Orbstack å®¿ä¸»æœºçš„ Ollama
client = OpenAI(
    base_url=os.getenv("OPENAI_API_BASE", "http://host.docker.internal:11434/v1"),
    api_key="ollama" # Ollama ä¸éœ€è¦çœŸå® Key
)

# 2. è·å–æ¨¡å‹åç§°ï¼ˆé»˜è®¤ç”¨ä½ é…ç½®çš„ qwen3ï¼‰
MODEL_NAME = os.getenv("MODEL_NAME", "qwen3-coder:30b")

# å¢åŠ ï¼šæ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– System Promptï¼Œé»˜è®¤ä¸ºåŸæ¥çš„é…ç½®
SYSTEM_PROMPT = os.getenv("SYSTEM_PROMPT", "ä½ æ˜¯ä¸€ä¸ªè¿è¡Œåœ¨Dockerå®¹å™¨é‡Œçš„èµ„æ·±å¤§æ•°æ®å¼€å‘åŠ©æ‰‹ã€‚è¯·ç›´æ¥ç”ŸæˆShellå‘½ä»¤æˆ–Pythonä»£ç ï¼Œä¸è¦åºŸè¯ã€‚")

def chat(prompt):
    print(f"ğŸ¤– æ­£åœ¨æ€è€ƒ (Model: {MODEL_NAME})...")
    print("-" * 40)
    
    try:
        # 3. å‘é€è¯·æ±‚ç»™ Ollama
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT}, # ä¿®æ”¹è¿™é‡Œ
                {"role": "user", "content": prompt}
            ],
            stream=True # æµå¼è¾“å‡ºï¼Œåƒæ‰“å­—æœºä¸€æ ·
        )
        
        # 4. æ‰“å°ç»“æœ
        full_content = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_content += content
        print("\n" + "-" * 40)
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ Ollama æ˜¯å¦åœ¨ Mac ä¸Šè¿è¡Œï¼Œä¸”æ‰§è¡Œäº† 'launchctl setenv OLLAMA_HOST 0.0.0.0'")

if __name__ == "__main__":
    # ç®€å•çš„å‘½ä»¤è¡Œå‚æ•°å¤„ç†
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: ai 'ä½ çš„æŒ‡ä»¤'")
        sys.exit(1)
    
    user_prompt = " ".join(sys.argv[1:])
    chat(user_prompt)